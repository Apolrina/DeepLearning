{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7297567,"sourceType":"datasetVersion","datasetId":4233106},{"sourceId":7312059,"sourceType":"datasetVersion","datasetId":4242979}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bert_score \n!pip install peft\n!pip install accelerate\n!pip install transformers==4.32.0","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:55:03.719501Z","iopub.execute_input":"2023-12-31T07:55:03.719866Z","iopub.status.idle":"2023-12-31T07:55:51.951863Z","shell.execute_reply.started":"2023-12-31T07:55:03.719839Z","shell.execute_reply":"2023-12-31T07:55:51.950864Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: bert_score in /opt/conda/lib/python3.10/site-packages (0.3.13)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.0.0)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.0.3)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.32.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bert_score) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert_score) (2.31.0)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert_score) (4.66.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert_score) (3.7.4)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert_score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert_score) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert_score) (2023.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert_score) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.19.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (2023.8.8)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert_score) (0.4.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert_score) (10.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert_score) (2023.11.17)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=3.0.0->bert_score) (2023.12.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.32.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.1)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.19.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.8.8)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.19.4)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nRequirement already satisfied: transformers==4.32.0 in /opt/conda/lib/python3.10/site-packages (4.32.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.32.0) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.32.0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.32.0) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.32.0) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip install -i https://test.pypi.org/simple/ bitsandbytes-cuda113 # version with GPU support.\n#!pip install bitsandbytes-cuda110 bitsandbytes # version with GPU support.\n!pip install -i https://test.pypi.org/simple/ bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2023-12-31T07:55:51.953857Z","iopub.execute_input":"2023-12-31T07:55:51.954188Z","iopub.status.idle":"2023-12-31T07:56:03.973162Z","shell.execute_reply.started":"2023-12-31T07:55:51.954147Z","shell.execute_reply":"2023-12-31T07:56:03.972127Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Looking in indexes: https://test.pypi.org/simple/\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.39.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import codecs\nimport copy\nimport gc\nimport json\nimport logging\nimport math\nimport os\nimport random\nimport sys\nfrom collections import defaultdict\nfrom typing import Dict, List, Set, Tuple, Union\n\nimport bert_score\nimport numpy as np\nimport torch\nfrom accelerate import infer_auto_device_map\nfrom bitsandbytes.optim import Adam8bit\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, get_peft_model, TaskType\nfrom tqdm import trange, tqdm\nfrom transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForSeq2SeqLM\nfrom transformers import BertModel, BertTokenizer\nfrom pathlib import Path\n\n# os.environ['TRANSFORMERS_OFFLINE'] = '1'\n\nllm_training_logger = logging.getLogger(__name__)\nDEFAULT_RANDOM_SEED = 44\nMAX_EPOCHS: int = 20\nbert_minibatch_size = 2\nminibatch_size = 2\nlearning_rate = 5e-5\npenalty_weight = 10000.0\nl2_regularizer_weight = 0.1\naccumulate_gradients = 1\nuse_birm = False\nmax_seq_len = None\nbert_num_layers = 9\n\ninput_model_name = \"ai-forever/FRED-T5-1.7B\"\ndata_name = \"training_dataset.jsonl\"\ndata_path = Path('/kaggle/input/trains-dataset/')\n\ntest = data_path / data_name\ntrain = data_path / data_name\n\ndef mean_pooling(model_output: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n    token_embeddings = model_output[0]\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings / sum_mask\n\ndef generate_bert_embeddings(texts: List[str], tokenizer: BertTokenizer, model: BertModel,\n                             minibatch: int) -> np.ndarray:\n    n_batches = math.ceil(len(texts) / minibatch)\n    res = []\n    for batch_idx in trange(n_batches):\n        batch_start = batch_idx * minibatch\n        batch_end = min(batch_start + minibatch, len(texts))\n        inputs = tokenizer(\n            [cur.lower() for cur in texts[batch_start:batch_end]],\n            max_length=512, padding='longest', truncation='longest_first', return_tensors='pt'\n        ).to(model.device)\n        with torch.no_grad():\n            model_outputs = model(**inputs)\n            sentence_embeddings = mean_pooling(model_outputs, inputs['attention_mask']).cpu().numpy()\n        if len(sentence_embeddings.shape) != 2:\n            err_msg = f'The sentence embedding shape is wrong! Expected 2, got {sentence_embeddings.shape}.'\n            llm_training_logger.error(err_msg)\n            raise ValueError(err_msg)\n        if sentence_embeddings.shape[0] != (batch_end - batch_start):\n            err_msg = (f'The first sentence embedding shape is wrong! Expected {batch_end - batch_start}, '\n                       f'got {sentence_embeddings.shape[0]}.')\n            llm_training_logger.error(err_msg)\n            raise ValueError(err_msg)\n        res.append(sentence_embeddings)\n    return np.vstack(res)\n\ndef generate_minibatch(dataset: Dict[str, List[Dict[str, List[int]]]], categories: List[str], minibatch: int,\n                       padding: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, Set[int]]:\n    if minibatch < 2:\n        err_msg = f'The mini-batch is too small! Expected 2 or greater, got {minibatch}.'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n    if minibatch < len(categories):\n        environments = random.sample(population=categories, k=minibatch)\n    elif minibatch == len(categories):\n        environments = categories\n    else:\n        environments = copy.copy(categories)\n        while len(environments) < minibatch:\n            environments.append(random.choice(categories))\n        pass\n    # environments.append(random.choice(categories))\n    input_ids = []\n    attention_mask = []\n    labels = []\n    environment_IDs = []\n    for env in environments:\n        sample = random.choice(dataset[env])\n        input_ids.append(torch.tensor(data=sample['input_ids'], dtype=torch.long))\n        attention_mask.append(torch.tensor(data=sample['attention_mask'], dtype=torch.long))\n        labels.append(torch.tensor(data=sample['labels'], dtype=torch.long))\n        environment_IDs.append(categories.index(env))\n    set_of_env = set(environment_IDs)\n    batched_input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=padding).cuda()\n    batched_attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0).cuda()\n    batched_labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True).cuda()\n    batched_environment_IDs = torch.tensor(data=environment_IDs, dtype=torch.long).cuda()\n    return batched_input_ids, batched_attention_mask, batched_labels, batched_environment_IDs, set_of_env\n\ndef predict(testset: List[Tuple[str, str, str]], tokenizer: AutoTokenizer, model: PeftModel,\n            minibatch: int) -> Tuple[List[str], List[str], List[str]]:\n    gen_config = model.generation_config\n    gen_config.max_new_tokens = 300\n    gen_config.do_sample = True\n    gen_config.top_k = 10\n    # gen_config.top_p = 0.9\n    # gen_config.num_return_sequences = 1\n    gen_config.pad_token_id = tokenizer.eos_token_id\n    gen_config.eos_token_id = tokenizer.eos_token_id\n    n_batches = math.ceil(len(testset) / minibatch)\n    true_answers = []\n    predicted_answers = []\n    input_prompts = []\n    for batch_idx in range(n_batches):\n        batch_start = batch_idx * minibatch\n        batch_end = min(len(testset), batch_start + minibatch)\n        input_ids = []\n        attention_mask = []\n        for sample_idx in range(batch_start, batch_end):\n            prompt = format_my_data(testset[sample_idx][0:2], True)\n            tokenized_text = tokenize_prompt(\n                prompt,\n                tokenizer,\n                add_labels=False\n            )\n            input_ids.append(torch.tensor(data=tokenized_text['input_ids'], dtype=torch.long))\n            attention_mask.append(torch.tensor(data=tokenized_text['attention_mask'], dtype=torch.long))\n            true_answers.append(testset[sample_idx][2])\n        batched_input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True,\n                                                            padding_value=tokenizer.pad_token_id).cuda()\n        batched_attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True,\n                                                                 padding_value=0).cuda()\n        with torch.no_grad():\n            generated_ids = model.generate(input_ids=batched_input_ids, attention_mask=batched_attention_mask,\n                                           generation_config=gen_config)\n        for sample_idx in range(batch_end - batch_start):\n            pass\n        predicted_answers += tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n        input_prompts += [cur.strip() for cur in tokenizer.batch_decode(input_ids, skip_special_tokens=True)]\n        del input_ids, attention_mask\n        del batched_input_ids, batched_attention_mask, generated_ids\n    if len(predicted_answers) != len(testset):\n        err_msg = f'The predicted answers do not correspond to the test set! {len(predicted_answers)} != {len(testset)}'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n    if len(true_answers) != len(testset):\n        err_msg = f'The true answers do not correspond to the test set! {len(true_answers)} != {len(testset)}'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n    return input_prompts, predicted_answers, true_answers\n\ndef strip_texts_for_bert(texts: List[str], tokenizer: BertTokenizer, max_len: int = 500) -> List[str]:\n    stripped_texts = []\n    for cur in texts:\n        tokenized = tokenizer.encode(cur.lower())\n        if len(tokenized) > max_len:\n            tokenized = tokenized[0:max_len]\n            stripped_texts.append(tokenizer.decode(token_ids=tokenized, skip_special_tokens=True))\n        else:\n            stripped_texts.append(cur.lower())\n    return stripped_texts\n\ndef prepare_model_for_bert_score(model: BertModel, num_layers: int) -> BertModel:\n    model.encoder.layer = torch.nn.ModuleList(\n        [layer for layer in model.encoder.layer[:num_layers]]\n    )\n    return model\n\ndef evaluate(questions: List[str], predicted_answers: List[str], true_answers: List[str],\n             tokenizer: BertTokenizer, model: BertModel,\n             minibatch: int) -> Tuple[float, List[Dict[str, Union[str, float]]]]:\n    if len(true_answers) != len(predicted_answers):\n        err_msg = f'The true answers do not correspond to the predicted answers! ' \\\n                  f'{len(true_answers)} != {len(predicted_answers)}.'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n    true_answers_ = strip_texts_for_bert(true_answers, tokenizer)\n    predicted_answers_ = strip_texts_for_bert(predicted_answers, tokenizer)\n    # idf_dict = bert_score.get_idf_dict(true_answers_, tokenizer, nthreads=max(1, os.cpu_count()))\n    idf_dict = defaultdict(lambda: 1.0)\n    idf_dict[tokenizer.sep_token_id] = 0\n    idf_dict[tokenizer.cls_token_id] = 0\n    all_preds = bert_score.bert_cos_score_idf(\n        model,\n        true_answers_,\n        predicted_answers_,\n        tokenizer,\n        idf_dict,\n        device=model.device,\n        batch_size=minibatch,\n        all_layers=False,\n    ).cpu()\n    f1_list = all_preds[..., 2].numpy().tolist()\n    if len(true_answers) != len(f1_list):\n        err_msg = f'The true answers do not correspond to the BERT scores! {len(true_answers)} != {len(f1_list)}.'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n    f1_mean = float(np.mean(f1_list))\n    res = []\n    for pred_, true_, f1_val, question in zip(predicted_answers, true_answers, f1_list, questions):\n        res.append({\n            'PROMPT': question,\n            'TRUE': true_,\n            'PRED': pred_,\n            'F1': f1_val\n        })\n    return f1_mean, sorted(res, key=lambda it: it['F1'])\n\ndef add_epoch_to_report_fname(report_fname: str, epoch: int) -> str:\n    if not report_fname.lower().endswith('.json'):\n        err_msg = f'The report file name = {os.path.basename(report_fname)} is wrong, because it is not JSON!'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n    new_report_fname = report_fname[:-5] + '_epoch{0:>03}'.format(epoch) + '.json'\n    return new_report_fname\n\ndef tokenize_prompt(prompt: str, tokenizer: AutoTokenizer, add_eos_token: bool = True,\n                    add_labels: bool = True) -> Dict[str, List[int]]:\n    result = tokenizer(prompt, padding=False, return_tensors=None)\n    if (result['input_ids'][-1] != tokenizer.eos_token_id) and add_eos_token:\n        result['input_ids'].append(tokenizer.eos_token_id)\n        result['attention_mask'].append(1)\n    if add_labels:\n        result['labels'] = result['input_ids'].copy()\n    return result\n\ndef format_my_data(sample: Union[Tuple[str, str, str], Tuple[str, str]], is_instruction_first: bool) -> str:\n    instruction = f'{sample[0]}'.strip()\n    context = f'{sample[1]}'.strip()\n\n    # prompt = 'Прочитай текст: ' + context + ' Теперь на основе текста ' + instruction\n    prompt = '<LM>Город, ' + instruction\n    prompt = ' '.join(prompt.split()).strip()\n    if len(prompt) == 0:\n        err_msg = f'{sample}: The instruction and context are empty!'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n    if len(sample) > 2:\n        response = f'{sample[2].strip()}'\n        if len(response) == 0:\n            err_msg = f'{sample}: The response is empty!'\n            llm_training_logger.error(err_msg)\n            raise ValueError(err_msg)\n        prompt += (' ' + response + '</s>')\n    return prompt\n\ndef generate_and_tokenize_prompt(data_point: Tuple[str, str, str], is_instruction_first: bool,\n                                 tokenizer: AutoTokenizer) -> Dict[str, List[int]]:\n    full_prompt = format_my_data(data_point, is_instruction_first)\n    tokenized_full_prompt = tokenize_prompt(full_prompt, tokenizer)\n    user_prompt = format_my_data(data_point[0:2], is_instruction_first)\n    tokenized_user_prompt = tokenize_prompt(user_prompt, tokenizer)\n    user_prompt_len = len(tokenized_user_prompt['input_ids'])\n    user_prompt_len -= 1\n    tokenized_full_prompt['labels'] = tokenized_full_prompt['labels'][user_prompt_len:]\n    tokenized_full_prompt['attention_mask'] = tokenized_full_prompt['attention_mask'][:user_prompt_len]\n    tokenized_full_prompt[\"input_ids\"] = tokenized_full_prompt['input_ids'][:user_prompt_len]\n    return tokenized_full_prompt\n\ndef print_trainable_parameters(model: PeftModel):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    info_msg = (f'Trainable parameters: {trainable_params}, all parameters: {all_param}, '\n                f'trainable %: {100 * trainable_params / all_param}')\n    llm_training_logger.info(info_msg)\n\ndef tokenize_dataset(dataset: Dict[str, List[Tuple[str, str, str]]], tokenizer: AutoTokenizer) -> \\\n        Dict[str, List[Dict[str, List[int]]]]:\n    tokenized = dict()\n    for category in sorted(list(dataset.keys())):\n        tokenized_subset = []\n        llm_training_logger.info(f'Texts of the category {category} are tokenized.')\n        for sample in tqdm(dataset[category]):\n            tokenized_subset.append(generate_and_tokenize_prompt(sample, False, tokenizer))\n            # if sample[1] is not None:\n            #    if len(sample[1]) > 0:\n            #        tokenized_subset.append(generate_and_tokenize_prompt(sample, False, tokenizer))\n        tokenized[category] = tokenized_subset\n        del tokenized_subset\n    return tokenized\n\ndef split_dolly_dataset(dataset: Dict[str, List[Tuple[str, str, str]]],\n                        emb_tokenizer: BertTokenizer, emb_model: BertModel, minibatch: int,\n                        random_state: int = None) -> (\n        Tuple)[Dict[str, List[Tuple[str, str, str]]], Dict[str, List[Tuple[str, str, str]]]]:\n    trainset = dict()\n    testset = dict()\n    for category in sorted(list(dataset.keys())):\n        train_indices = []\n        test_indices = []\n        for i in range(0, 66):\n            train_indices.append(i)\n        for i in range(66, 84):\n            test_indices.append(i)\n        trainset[category] = []\n        testset[category] = []\n        for idx in train_indices:\n            trainset[category].append(dataset[category][idx])\n        for idx in test_indices:\n            testset[category].append(dataset[category][idx])\n        del train_indices, test_indices\n        gc.collect()\n    return trainset, testset\n\ndef load_bert(bert_name: str) -> Tuple[BertTokenizer, BertModel]:\n    bert_name = \"DeepPavlov/rubert-base-cased\"\n    tokenizer = BertTokenizer.from_pretrained(bert_name)\n    model = BertModel.from_pretrained(bert_name).cuda()\n    model.eval()\n    llm_training_logger.info(f'The BERT model {bert_name} is loaded.')\n    return tokenizer, model\n\ndef create_peft_model(llm: AutoModelForSeq2SeqLM, lora_r: int = 64, lora_alpha: int = 128,\n                      lora_dropout: float = 0.05) -> PeftModel:\n    config = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=lora_dropout,\n        target_modules=[\"q\", \"v\"],\n        bias='none',\n        task_type=TaskType.SEQ_2_SEQ_LM,\n    )\n    return get_peft_model(llm, config)\n\ndef load_my_data_dataset(dataset_name: str) -> Dict[str, List[Tuple[str, str, str]]]:\n    res = dict()\n    dataset = load_dataset(\"json\", data_files= str(train))\n    dataset_splits = list(dataset.keys())\n    print(len(dataset_splits))\n    info_msg = f'The dataset \"{dataset_name}\" is loaded.'\n    if len(dataset_splits) > 1:\n        info_msg += f' There are {len(dataset_splits)} splits: {dataset_splits}.'\n    else:\n        if len(dataset_splits) < 1:\n            err_msg = f'The dataset \"{dataset_name}\" contains no data splits!'\n            llm_training_logger.error(err_msg)\n            raise ValueError(err_msg)\n        info_msg += f' There is 1 split: {dataset_splits}.'\n    llm_training_logger.info(info_msg)\n    true_sample_keys = {'instruction', 'context', 'response', 'category'}\n    for cur_split in dataset_splits:\n        print(enumerate(dataset[cur_split]))\n        for sample_idx, cur_sample in enumerate(dataset[cur_split]):\n            if not isinstance(cur_sample, dict):\n                err_msg = (f'The sample {sample_idx} of the split {cur_split} has a wrong type! '\n                           f'Expected {type({\"a\": 1})}, got {type(cur_sample)}.')\n                llm_training_logger.error(err_msg)\n                raise ValueError(err_msg)\n            sample_keys = set(cur_sample.keys())\n            if sample_keys != true_sample_keys:\n                err_msg = (f'The sample {sample_idx} of the split {cur_split} has wrong fields! '\n                           f'Expected {sorted(list(true_sample_keys))}, got {sorted(list(sample_keys))}.')\n                llm_training_logger.error(err_msg)\n                raise ValueError(err_msg)\n            new_category = cur_sample['category'].strip()\n            if len(new_category) == 0:\n                err_msg = f'The sample {sample_idx} of the split {cur_split} has an empty category!'\n                llm_training_logger.error(err_msg)\n                raise ValueError(err_msg)\n            if new_category not in res:\n                res[new_category] = []\n            instruction = cur_sample['instruction'].strip()\n            if len(instruction) == 0:\n                err_msg = f'The sample {sample_idx} of the split {cur_split} has an empty instruction!'\n                llm_training_logger.error(err_msg)\n                raise ValueError(err_msg)\n            response = cur_sample['response'].strip()\n            if len(response) == 0:\n                err_msg = f'The sample {sample_idx} of the split {cur_split} has an empty response!'\n                llm_training_logger.error(err_msg)\n                raise ValueError(err_msg)\n            context = cur_sample['context'].strip()\n\n            res[new_category].append((\n                ' '.join(instruction.split()).strip(),\n                ' '.join(context.split()).strip(),\n                ' '.join(response.split()).strip()\n            ))\n\n    all_categories = sorted(list(res.keys()))\n    if len(all_categories) == 0:\n        err_msg = f'The dataset \"{dataset_name}\" is empty!'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n    info_msg = f'There are {len(all_categories)} categories in the dataset \"{dataset_name}\". They are:'\n    llm_training_logger.info(info_msg)\n    max_width = len(all_categories[0])\n    for cur in all_categories[1:]:\n        if len(cur) > max_width:\n            max_width = len(cur)\n    for cur in all_categories[:-1]:\n        llm_training_logger.info('  - {0:<{1}}: {2:>5} samples;'.format(cur + ':', max_width + 1, len(res[cur])))\n    cur = all_categories[-1]\n    llm_training_logger.info('  - {0:<{1}}: {2:>5} samples.'.format(cur + ':', max_width + 1, len(res[cur])))\n    del dataset\n    return res\n\ndef load_model_and_tokenizer(model_name: str) -> Tuple[AutoTokenizer, AutoModelForSeq2SeqLM]:\n    tokenizer = AutoTokenizer.from_pretrained(model_name, eos_token='</s>', skip_special_tokens=True,\n                                              trust_remote_code=True)  # E\n\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=False,\n    )\n\n    device_map = 'auto'\n\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        model_name,\n        device_map=device_map,\n        quantization_config=bnb_config,\n        # offload_folder=\"offload\",\n        trust_remote_code=True,\n        # offload_state_dict=True,\n        torch_dtype=torch.float16\n    )\n    device_map = infer_auto_device_map(model)\n    print(device_map)\n    print(model.get_memory_footprint())\n    print(model)\n    return tokenizer, model\n\ndef main(l2_regularizer_weight=0.1):\n    print(f\"Is CUDA supported by this system?  {torch.cuda.is_available()}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    if not torch.cuda.is_available():\n        err_msg = 'CUDA is not available!'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n\n    random.seed(DEFAULT_RANDOM_SEED)\n    torch.manual_seed(DEFAULT_RANDOM_SEED)\n    np.random.seed(DEFAULT_RANDOM_SEED)\n    torch.cuda.manual_seed(DEFAULT_RANDOM_SEED)\n\n    output_model_name = os.path.normpath(\"my_model\")\n    if os.path.basename(output_model_name).lower() == os.path.basename(input_model_name).lower():\n        err_msg = 'The input model name and the model output name are same!'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n    if not os.path.isdir(output_model_name):\n        base_dir = os.path.dirname(output_model_name)\n        if len(base_dir) > 0:\n            if not os.path.isdir(base_dir):\n                err_msg = f'The directory \"{base_dir}\" does not exist!'\n                llm_training_logger.error(err_msg)\n                raise ValueError(err_msg)\n        os.mkdir(output_model_name)\n\n    report_fname = os.path.normpath(\"report.json\")\n    if not os.path.isfile(report_fname):\n        base_dir = os.path.dirname(report_fname)\n        if len(base_dir) > 0:\n            if not os.path.isdir(base_dir):\n                err_msg = f'The directory \"{base_dir}\" does not exist!'\n                llm_training_logger.error(err_msg)\n                raise ValueError(err_msg)\n\n    bert_name = os.path.normpath(\"DeepPavlov/rubert-base-cased\")\n\n    my_model_tokenizer, my_model = load_model_and_tokenizer(input_model_name)\n\n    my_model = create_peft_model(my_model)\n\n    full_my_data = load_my_data_dataset(dataset_name=data_name)\n\n    all_categories = sorted(list(full_my_data.keys()))\n\n    bert_tokenizer, bert_model = load_bert(bert_name)\n\n    my_data_for_training, my_data_for_testing = split_dolly_dataset(\n        dataset=full_my_data,\n        emb_tokenizer=bert_tokenizer,\n        emb_model=bert_model,\n        minibatch=bert_minibatch_size,\n        random_state=random.randint(0, 2147483647)\n    )\n\n    if set(my_data_for_testing.keys()) != set(my_data_for_training.keys()):\n        err_msg = 'The training categories do not correspond to the testing categories.'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n    if set(my_data_for_testing.keys()) != set(all_categories):\n        err_msg = 'The training categories do not correspond to the testing categories.'\n        llm_training_logger.error(err_msg)\n        raise ValueError(err_msg)\n    n_samples_for_training = len(my_data_for_training[all_categories[0]])\n    for category in all_categories[1:]:\n        n_samples_for_training += len(my_data_for_training[category])\n\n    gc.collect()\n\n    tokenized_my_data_for_training = tokenize_dataset(my_data_for_training, my_model_tokenizer)\n\n    gc.collect()\n\n    print_trainable_parameters(my_model)\n    my_model.eval()\n\n    validation_true = []\n    validation_pred = []\n    validation_questions = []\n    for category in tqdm(all_categories):\n        prompt_, pred_, true_ = predict(my_data_for_testing[category], my_model_tokenizer, my_model,\n                                        minibatch_size)\n        validation_pred += pred_\n        validation_true += true_\n        validation_questions += prompt_\n        del pred_, true_, prompt_\n    bert_model = prepare_model_for_bert_score(bert_model, num_layers=bert_num_layers)\n    best_f1, detailed_validation_report = evaluate(validation_questions, validation_pred, validation_true,\n                                                   bert_tokenizer, bert_model, bert_minibatch_size)\n    llm_training_logger.info(f'Before training: validation BERT F1 = {best_f1}.')\n    with codecs.open(add_epoch_to_report_fname(report_fname, 0), mode='w', encoding='utf-8') as fp:\n        json.dump(\n            obj={\n                'total': {'f1': best_f1},\n                'detailed': detailed_validation_report\n            },\n            fp=fp,\n            ensure_ascii=False,\n            indent=4\n        )\n    with codecs.open(report_fname, mode='w', encoding='utf-8') as fp:\n        json.dump(\n            obj={\n                'total': {'f1': best_f1},\n                'detailed': detailed_validation_report\n            },\n            fp=fp,\n            ensure_ascii=False,\n            indent=4\n        )\n    del detailed_validation_report, validation_pred, validation_true\n\n    optimizer = Adam8bit(\n        my_model.parameters(),\n        lr=learning_rate,\n    )\n\n    n_training_batches = int(np.ceil(n_samples_for_training / minibatch_size))\n    if accumulate_gradients > 1:\n        while (n_training_batches % accumulate_gradients) != 0:\n            n_training_batches += 1\n    llm_training_logger.info(f'Iterations per epoch is {n_training_batches}.')\n\n    if use_birm:\n        loss_fct = torch.nn.CrossEntropyLoss().cuda()\n\n        with torch.no_grad():\n            weight_norm = torch.tensor(0.).cuda()\n            for w in my_model.parameters():\n                if w.requires_grad:\n                    weight_norm += w.norm().pow(2)\n            weight_norm_val = float(weight_norm.detach().cpu())\n        n = 1.0\n        weight_norm_val_ = weight_norm_val\n        while weight_norm_val_ > 1.0:\n            n *= 10.0\n            weight_norm_val_ /= 10.0\n        l2_regularizer_weight = l2_regularizer_weight / n\n        info_msg = (f'BIRM is used. The weight norm is {weight_norm_val}, '\n                    f'and L2 regularizer weight is {l2_regularizer_weight}.')\n        llm_training_logger.info(info_msg)\n    else:\n        loss_fct = None\n        llm_training_logger.info('ERM is used.')\n        l2_regularizer_weight = 0.0\n\n    torch.cuda.empty_cache()\n    del full_my_data\n    for epoch in range(1, MAX_EPOCHS + 1):\n        llm_training_logger.info(f'Epoch {epoch} is started.')\n        total_training_loss_val = 0.0\n        training_fct_loss_val = 0.0\n        weight_norm_val = 0.0\n        training_penalty_val = 0.0\n        my_model.train()\n        for iter_idx in trange(1, n_training_batches + 1):\n            input_ids, attention_mask, labels, environments, set_of_env = generate_minibatch(\n                dataset=tokenized_my_data_for_training,\n                categories=all_categories,\n                minibatch=minibatch_size,\n                padding=my_model_tokenizer.pad_token_id\n            )\n            res = my_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels, return_dict=True)\n            if use_birm:\n                train_logits = res.logits\n                train_nll = res.loss\n                training_fct_loss_val += train_nll.detach().cpu()\n                loss_list = []\n                for env_ID in set_of_env:\n                    ei = (environments == env_ID).view(-1)\n                    labels_for_env = labels[ei]\n                    logits_for_env = train_logits[ei]\n                    shift_logits = logits_for_env[..., :-1, :].contiguous()\n                    shift_labels = labels_for_env[..., 1:].contiguous()\n                    batch_size, seq_length, vocab_size = shift_logits.shape\n                    train_nll_ = loss_fct(\n                        shift_logits.view(batch_size * seq_length, vocab_size),\n                        shift_labels.view(batch_size * seq_length)\n                    )\n                    loss_list.append(train_nll_)\n                loss_t = torch.stack(loss_list)\n                train_penalty = ((loss_t - loss_t.mean()) ** 2).mean()\n                training_penalty_val += train_penalty.detach().cpu()\n                weight_norm = torch.tensor(0.).cuda()\n                for w in my_model.parameters():\n                    if w.requires_grad:\n                        weight_norm += w.norm().pow(2)\n                weight_norm_val += float(weight_norm.detach().cpu())\n                loss = train_nll.clone()\n                loss += l2_regularizer_weight * weight_norm\n                loss += penalty_weight * train_penalty\n                total_training_loss_val += loss.detach().cpu()\n            else:\n                # print(res)\n                loss = res.loss\n                instant_loss = loss.detach().cpu().numpy()\n                total_training_loss_val += instant_loss\n            if accumulate_gradients > 1:\n                loss = loss / accumulate_gradients\n            loss.backward()\n            if accumulate_gradients > 1:\n                if iter_idx % accumulate_gradients == 0:\n                    torch.nn.utils.clip_grad_norm_(my_model.parameters(), 1.0)\n                    optimizer.step()\n                    optimizer.zero_grad()\n            else:\n                torch.nn.utils.clip_grad_norm_(my_model.parameters(), 1.0)\n                optimizer.step()\n                optimizer.zero_grad()\n            del input_ids, attention_mask, labels, res, environments, set_of_env\n        print(total_training_loss_val)\n        total_training_loss_val /= float(n_training_batches / accumulate_gradients)\n        training_fct_loss_val /= float(n_training_batches / accumulate_gradients)\n        weight_norm_val /= float(n_training_batches / accumulate_gradients)\n        training_penalty_val /= float(n_training_batches / accumulate_gradients)\n        if use_birm:\n            info_msg = (f'Epoch {epoch}: total training loss is {total_training_loss_val}, '\n                        f'training cross-entropy is {training_fct_loss_val}, '\n                        f'training penalty is {training_penalty_val}, weight norm is {weight_norm_val}.')\n        else:\n            info_msg = f'Epoch {epoch}: training loss is {total_training_loss_val}.'\n        llm_training_logger.info(info_msg)\n        my_model.eval()\n        validation_true = []\n        validation_pred = []\n        for category in tqdm(all_categories):\n            _, pred_, true_ = predict(my_data_for_testing[category], my_model_tokenizer, my_model, minibatch_size)\n            validation_pred += pred_\n            validation_true += true_\n            del pred_, true_\n        new_f1, detailed_validation_report = evaluate(validation_questions, validation_pred, validation_true,\n                                                      bert_tokenizer, bert_model, bert_minibatch_size)\n        llm_training_logger.info(f'Epoch {epoch}: validation BERT F1 = {new_f1}.')\n        if new_f1 > best_f1:\n            best_f1 = new_f1\n            with codecs.open(report_fname, mode='w', encoding='utf-8') as fp:\n                json.dump(\n                    obj={\n                        'total': {'f1': best_f1},\n                        'detailed': detailed_validation_report\n                    },\n                    fp=fp,\n                    ensure_ascii=False,\n                    indent=4\n                )\n            my_model.save_pretrained(output_model_name)\n            llm_training_logger.info(f'The model is updated with F1 = {best_f1}.')\n        with codecs.open(add_epoch_to_report_fname(report_fname, epoch), mode='w', encoding='utf-8') as fp:\n            json.dump(\n                obj={\n                    'total': {'f1': new_f1},\n                    'detailed': detailed_validation_report\n                },\n                fp=fp,\n                ensure_ascii=False,\n                indent=4\n            )\n        del detailed_validation_report, validation_pred, validation_true\n        llm_training_logger.info(f'Epoch {epoch} is finished.')\n\nif __name__ == '__main__':\n    llm_training_logger.setLevel(logging.INFO)\n    fmt_str = '%(filename)s[LINE:%(lineno)d]# %(levelname)-8s ' \\\n              '[%(asctime)s]  %(message)s'\n    formatter = logging.Formatter(fmt_str)\n    stdout_handler = logging.StreamHandler(sys.stdout)\n    stdout_handler.setFormatter(formatter)\n    llm_training_logger.addHandler(stdout_handler)\n    file_handler = logging.FileHandler('llm_training.log')\n    file_handler.setFormatter(formatter)\n    llm_training_logger.addHandler(file_handler)\n    main()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T08:03:55.265171Z","iopub.execute_input":"2023-12-31T08:03:55.265546Z","iopub.status.idle":"2023-12-31T08:19:57.530489Z","shell.execute_reply.started":"2023-12-31T08:03:55.265518Z","shell.execute_reply":"2023-12-31T08:19:57.529560Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Is CUDA supported by this system?  True\nCUDA version: 11.8\n{'': 0}\n2159502336\nT5ForConditionalGeneration(\n  (shared): Embedding(50364, 1536)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(50364, 1536)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (k): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (v): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (o): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (relative_attention_bias): Embedding(32, 24)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear4bit(in_features=1536, out_features=4096, bias=False)\n              (wi_1): Linear4bit(in_features=1536, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (k): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (v): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (o): Linear4bit(in_features=1536, out_features=1536, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear4bit(in_features=1536, out_features=4096, bias=False)\n              (wi_1): Linear4bit(in_features=1536, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(50364, 1536)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (k): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (v): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (o): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (relative_attention_bias): Embedding(32, 24)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (k): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (v): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (o): Linear4bit(in_features=1536, out_features=1536, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear4bit(in_features=1536, out_features=4096, bias=False)\n              (wi_1): Linear4bit(in_features=1536, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (k): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (v): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (o): Linear4bit(in_features=1536, out_features=1536, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (k): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (v): Linear4bit(in_features=1536, out_features=1536, bias=False)\n              (o): Linear4bit(in_features=1536, out_features=1536, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear4bit(in_features=1536, out_features=4096, bias=False)\n              (wi_1): Linear4bit(in_features=1536, out_features=4096, bias=False)\n              (wo): Linear(in_features=4096, out_features=1536, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=1536, out_features=50364, bias=False)\n)\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-c10fb2d1d2b86574/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75c403580cd24ac8b6c33512c0425a83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18e3650d54564289b058cae8229ee8fa"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-c10fb2d1d2b86574/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8c7c97259774d6ca1f8cb6f83cb9e8f"}},"metadata":{}},{"name":"stdout","text":"1\n2495533887.py[LINE:356]# INFO     [2023-12-31 08:04:05,507]  The dataset \"training_dataset.jsonl\" is loaded. There is 1 split: ['train'].\n2495533887.py[LINE:356]# INFO     [2023-12-31 08:04:05,507]  The dataset \"training_dataset.jsonl\" is loaded. There is 1 split: ['train'].\n2495533887.py[LINE:356]# INFO     [2023-12-31 08:04:05,507]  The dataset \"training_dataset.jsonl\" is loaded. There is 1 split: ['train'].\n<enumerate object at 0x78704d396cc0>\n2495533887.py[LINE:403]# INFO     [2023-12-31 08:04:05,519]  There are 1 categories in the dataset \"training_dataset.jsonl\". They are:\n2495533887.py[LINE:403]# INFO     [2023-12-31 08:04:05,519]  There are 1 categories in the dataset \"training_dataset.jsonl\". They are:\n2495533887.py[LINE:403]# INFO     [2023-12-31 08:04:05,519]  There are 1 categories in the dataset \"training_dataset.jsonl\". They are:\n2495533887.py[LINE:411]# INFO     [2023-12-31 08:04:05,521]    - creative_writing::    84 samples.\n2495533887.py[LINE:411]# INFO     [2023-12-31 08:04:05,521]    - creative_writing::    84 samples.\n2495533887.py[LINE:411]# INFO     [2023-12-31 08:04:05,521]    - creative_writing::    84 samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/1.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50d28730987948b18461295dbfc6b3da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1027d3841bec48919d97d376d96b9182"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39bb3a1570ff46e982d7316aa2fe082f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cb34b02819c43109a5fa769ae4a97bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"671a631804bb4f08a94a4befdd350d7f"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:327]# INFO     [2023-12-31 08:04:13,085]  The BERT model DeepPavlov/rubert-base-cased is loaded.\n2495533887.py[LINE:327]# INFO     [2023-12-31 08:04:13,085]  The BERT model DeepPavlov/rubert-base-cased is loaded.\n2495533887.py[LINE:327]# INFO     [2023-12-31 08:04:13,085]  The BERT model DeepPavlov/rubert-base-cased is loaded.\n2495533887.py[LINE:289]# INFO     [2023-12-31 08:04:13,505]  Texts of the category creative_writing are tokenized.\n2495533887.py[LINE:289]# INFO     [2023-12-31 08:04:13,505]  Texts of the category creative_writing are tokenized.\n2495533887.py[LINE:289]# INFO     [2023-12-31 08:04:13,505]  Texts of the category creative_writing are tokenized.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 66/66 [00:00<00:00, 1242.58it/s]","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:282]# INFO     [2023-12-31 08:04:13,790]  Trainable parameters: 28311552, all parameters: 1126937088, trainable %: 2.5122566558036645\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:282]# INFO     [2023-12-31 08:04:13,790]  Trainable parameters: 28311552, all parameters: 1126937088, trainable %: 2.5122566558036645\n2495533887.py[LINE:282]# INFO     [2023-12-31 08:04:13,790]  Trainable parameters: 28311552, all parameters: 1126937088, trainable %: 2.5122566558036645\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:32<00:00, 32.35s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:533]# INFO     [2023-12-31 08:04:46,458]  Before training: validation BERT F1 = 0.4913090169429779.\n2495533887.py[LINE:533]# INFO     [2023-12-31 08:04:46,458]  Before training: validation BERT F1 = 0.4913090169429779.\n2495533887.py[LINE:533]# INFO     [2023-12-31 08:04:46,458]  Before training: validation BERT F1 = 0.4913090169429779.\n2495533887.py[LINE:565]# INFO     [2023-12-31 08:04:46,477]  Iterations per epoch is 33.\n2495533887.py[LINE:565]# INFO     [2023-12-31 08:04:46,477]  Iterations per epoch is 33.\n2495533887.py[LINE:565]# INFO     [2023-12-31 08:04:46,477]  Iterations per epoch is 33.\n2495533887.py[LINE:587]# INFO     [2023-12-31 08:04:46,480]  ERM is used.\n2495533887.py[LINE:587]# INFO     [2023-12-31 08:04:46,480]  ERM is used.\n2495533887.py[LINE:587]# INFO     [2023-12-31 08:04:46,480]  ERM is used.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:04:46,489]  Epoch 1 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:04:46,489]  Epoch 1 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:04:46,489]  Epoch 1 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:15<00:00,  2.19it/s]","output_type":"stream"},{"name":"stdout","text":"329.89453125\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:05:01,570]  Epoch 1: training loss is 9.996803977272727.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:05:01,570]  Epoch 1: training loss is 9.996803977272727.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:05:01,570]  Epoch 1: training loss is 9.996803977272727.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:19<00:00, 19.48s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:05:21,338]  Epoch 1: validation BERT F1 = 0.5036631955040826.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:05:21,338]  Epoch 1: validation BERT F1 = 0.5036631955040826.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:05:21,338]  Epoch 1: validation BERT F1 = 0.5036631955040826.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:05:21,604]  The model is updated with F1 = 0.5036631955040826.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:05:21,604]  The model is updated with F1 = 0.5036631955040826.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:05:21,604]  The model is updated with F1 = 0.5036631955040826.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:05:21,608]  Epoch 1 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:05:21,608]  Epoch 1 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:05:21,608]  Epoch 1 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:05:21,611]  Epoch 2 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:05:21,611]  Epoch 2 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:05:21,611]  Epoch 2 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:15<00:00,  2.11it/s]","output_type":"stream"},{"name":"stdout","text":"213.19140625\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:05:37,259]  Epoch 2: training loss is 6.460345643939394.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:05:37,259]  Epoch 2: training loss is 6.460345643939394.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:05:37,259]  Epoch 2: training loss is 6.460345643939394.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:29<00:00, 29.99s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:06:07,546]  Epoch 2: validation BERT F1 = 0.5294411778450012.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:06:07,546]  Epoch 2: validation BERT F1 = 0.5294411778450012.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:06:07,546]  Epoch 2: validation BERT F1 = 0.5294411778450012.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:06:07,819]  The model is updated with F1 = 0.5294411778450012.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:06:07,819]  The model is updated with F1 = 0.5294411778450012.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:06:07,819]  The model is updated with F1 = 0.5294411778450012.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:06:07,824]  Epoch 2 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:06:07,824]  Epoch 2 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:06:07,824]  Epoch 2 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:06:07,827]  Epoch 3 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:06:07,827]  Epoch 3 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:06:07,827]  Epoch 3 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.26it/s]","output_type":"stream"},{"name":"stdout","text":"150.296875\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:06:22,444]  Epoch 3: training loss is 4.554450757575758.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:06:22,444]  Epoch 3: training loss is 4.554450757575758.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:06:22,444]  Epoch 3: training loss is 4.554450757575758.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [01:24<00:00, 84.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:07:47,618]  Epoch 3: validation BERT F1 = 0.5445684194564819.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:07:47,618]  Epoch 3: validation BERT F1 = 0.5445684194564819.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:07:47,618]  Epoch 3: validation BERT F1 = 0.5445684194564819.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:07:47,884]  The model is updated with F1 = 0.5445684194564819.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:07:47,884]  The model is updated with F1 = 0.5445684194564819.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:07:47,884]  The model is updated with F1 = 0.5445684194564819.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:07:47,889]  Epoch 3 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:07:47,889]  Epoch 3 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:07:47,889]  Epoch 3 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:07:47,891]  Epoch 4 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:07:47,891]  Epoch 4 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:07:47,891]  Epoch 4 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.28it/s]","output_type":"stream"},{"name":"stdout","text":"127.845703125\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:08:02,391]  Epoch 4: training loss is 3.874112215909091.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:08:02,391]  Epoch 4: training loss is 3.874112215909091.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:08:02,391]  Epoch 4: training loss is 3.874112215909091.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:20<00:00, 20.07s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:08:22,752]  Epoch 4: validation BERT F1 = 0.5375705460707346.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:08:22,752]  Epoch 4: validation BERT F1 = 0.5375705460707346.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:08:22,752]  Epoch 4: validation BERT F1 = 0.5375705460707346.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:08:22,756]  Epoch 4 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:08:22,756]  Epoch 4 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:08:22,756]  Epoch 4 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:08:22,759]  Epoch 5 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:08:22,759]  Epoch 5 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:08:22,759]  Epoch 5 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"118.810546875\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:08:37,052]  Epoch 5: training loss is 3.600319602272727.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:08:37,052]  Epoch 5: training loss is 3.600319602272727.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:08:37,052]  Epoch 5: training loss is 3.600319602272727.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:19<00:00, 19.73s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:08:57,069]  Epoch 5: validation BERT F1 = 0.5412630389134089.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:08:57,069]  Epoch 5: validation BERT F1 = 0.5412630389134089.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:08:57,069]  Epoch 5: validation BERT F1 = 0.5412630389134089.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:08:57,073]  Epoch 5 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:08:57,073]  Epoch 5 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:08:57,073]  Epoch 5 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:08:57,075]  Epoch 6 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:08:57,075]  Epoch 6 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:08:57,075]  Epoch 6 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.25it/s]","output_type":"stream"},{"name":"stdout","text":"116.796875\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:09:11,768]  Epoch 6: training loss is 3.539299242424242.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:09:11,768]  Epoch 6: training loss is 3.539299242424242.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:09:11,768]  Epoch 6: training loss is 3.539299242424242.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:20<00:00, 20.32s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:09:32,390]  Epoch 6: validation BERT F1 = 0.5342353714836968.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:09:32,390]  Epoch 6: validation BERT F1 = 0.5342353714836968.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:09:32,390]  Epoch 6: validation BERT F1 = 0.5342353714836968.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:09:32,394]  Epoch 6 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:09:32,394]  Epoch 6 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:09:32,394]  Epoch 6 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:09:32,397]  Epoch 7 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:09:32,397]  Epoch 7 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:09:32,397]  Epoch 7 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.29it/s]","output_type":"stream"},{"name":"stdout","text":"110.8701171875\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:09:46,796]  Epoch 7: training loss is 3.3597005208333335.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:09:46,796]  Epoch 7: training loss is 3.3597005208333335.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:09:46,796]  Epoch 7: training loss is 3.3597005208333335.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:25<00:00, 25.25s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:10:12,349]  Epoch 7: validation BERT F1 = 0.5384660065174103.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:10:12,349]  Epoch 7: validation BERT F1 = 0.5384660065174103.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:10:12,349]  Epoch 7: validation BERT F1 = 0.5384660065174103.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:10:12,354]  Epoch 7 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:10:12,354]  Epoch 7 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:10:12,354]  Epoch 7 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:10:12,356]  Epoch 8 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:10:12,356]  Epoch 8 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:10:12,356]  Epoch 8 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.34it/s]","output_type":"stream"},{"name":"stdout","text":"103.16796875\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:10:26,490]  Epoch 8: training loss is 3.1263020833333335.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:10:26,490]  Epoch 8: training loss is 3.1263020833333335.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:10:26,490]  Epoch 8: training loss is 3.1263020833333335.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:32<00:00, 32.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:10:59,059]  Epoch 8: validation BERT F1 = 0.5537660006019804.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:10:59,059]  Epoch 8: validation BERT F1 = 0.5537660006019804.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:10:59,059]  Epoch 8: validation BERT F1 = 0.5537660006019804.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:10:59,330]  The model is updated with F1 = 0.5537660006019804.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:10:59,330]  The model is updated with F1 = 0.5537660006019804.\n2495533887.py[LINE:690]# INFO     [2023-12-31 08:10:59,330]  The model is updated with F1 = 0.5537660006019804.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:10:59,335]  Epoch 8 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:10:59,335]  Epoch 8 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:10:59,335]  Epoch 8 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:10:59,337]  Epoch 9 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:10:59,337]  Epoch 9 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:10:59,337]  Epoch 9 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.28it/s]","output_type":"stream"},{"name":"stdout","text":"106.7216796875\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:11:13,818]  Epoch 9: training loss is 3.233990293560606.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:11:13,818]  Epoch 9: training loss is 3.233990293560606.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:11:13,818]  Epoch 9: training loss is 3.233990293560606.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [01:02<00:00, 62.84s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:12:16,966]  Epoch 9: validation BERT F1 = 0.5426027145650651.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:12:16,966]  Epoch 9: validation BERT F1 = 0.5426027145650651.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:12:16,966]  Epoch 9: validation BERT F1 = 0.5426027145650651.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:12:16,970]  Epoch 9 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:12:16,970]  Epoch 9 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:12:16,970]  Epoch 9 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:12:16,972]  Epoch 10 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:12:16,972]  Epoch 10 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:12:16,972]  Epoch 10 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.25it/s]","output_type":"stream"},{"name":"stdout","text":"104.2607421875\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:12:31,657]  Epoch 10: training loss is 3.159416429924242.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:12:31,657]  Epoch 10: training loss is 3.159416429924242.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:12:31,657]  Epoch 10: training loss is 3.159416429924242.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:23<00:00, 23.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:12:55,531]  Epoch 10: validation BERT F1 = 0.5253857788112428.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:12:55,531]  Epoch 10: validation BERT F1 = 0.5253857788112428.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:12:55,531]  Epoch 10: validation BERT F1 = 0.5253857788112428.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:12:55,535]  Epoch 10 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:12:55,535]  Epoch 10 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:12:55,535]  Epoch 10 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:12:55,538]  Epoch 11 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:12:55,538]  Epoch 11 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:12:55,538]  Epoch 11 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"95.9775390625\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:13:09,927]  Epoch 11: training loss is 2.908410274621212.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:13:09,927]  Epoch 11: training loss is 2.908410274621212.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:13:09,927]  Epoch 11: training loss is 2.908410274621212.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:25<00:00, 25.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:13:35,806]  Epoch 11: validation BERT F1 = 0.5327060719331106.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:13:35,806]  Epoch 11: validation BERT F1 = 0.5327060719331106.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:13:35,806]  Epoch 11: validation BERT F1 = 0.5327060719331106.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:13:35,810]  Epoch 11 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:13:35,810]  Epoch 11 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:13:35,810]  Epoch 11 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:13:35,813]  Epoch 12 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:13:35,813]  Epoch 12 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:13:35,813]  Epoch 12 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.30it/s]","output_type":"stream"},{"name":"stdout","text":"94.1865234375\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:13:50,200]  Epoch 12: training loss is 2.8541370738636362.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:13:50,200]  Epoch 12: training loss is 2.8541370738636362.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:13:50,200]  Epoch 12: training loss is 2.8541370738636362.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:22<00:00, 22.92s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:14:13,402]  Epoch 12: validation BERT F1 = 0.5406846387518777.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:14:13,402]  Epoch 12: validation BERT F1 = 0.5406846387518777.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:14:13,402]  Epoch 12: validation BERT F1 = 0.5406846387518777.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:14:13,406]  Epoch 12 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:14:13,406]  Epoch 12 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:14:13,406]  Epoch 12 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:14:13,408]  Epoch 13 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:14:13,408]  Epoch 13 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:14:13,408]  Epoch 13 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.29it/s]","output_type":"stream"},{"name":"stdout","text":"92.0888671875\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:14:27,868]  Epoch 13: training loss is 2.7905717329545454.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:14:27,868]  Epoch 13: training loss is 2.7905717329545454.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:14:27,868]  Epoch 13: training loss is 2.7905717329545454.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:30<00:00, 30.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:14:58,185]  Epoch 13: validation BERT F1 = 0.5407669610447354.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:14:58,185]  Epoch 13: validation BERT F1 = 0.5407669610447354.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:14:58,185]  Epoch 13: validation BERT F1 = 0.5407669610447354.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:14:58,189]  Epoch 13 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:14:58,189]  Epoch 13 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:14:58,189]  Epoch 13 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:14:58,192]  Epoch 14 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:14:58,192]  Epoch 14 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:14:58,192]  Epoch 14 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:15<00:00,  2.19it/s]","output_type":"stream"},{"name":"stdout","text":"93.255859375\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:15:13,255]  Epoch 14: training loss is 2.825935132575758.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:15:13,255]  Epoch 14: training loss is 2.825935132575758.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:15:13,255]  Epoch 14: training loss is 2.825935132575758.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:19<00:00, 19.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:15:32,976]  Epoch 14: validation BERT F1 = 0.5323542141252093.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:15:32,976]  Epoch 14: validation BERT F1 = 0.5323542141252093.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:15:32,976]  Epoch 14: validation BERT F1 = 0.5323542141252093.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:15:32,981]  Epoch 14 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:15:32,981]  Epoch 14 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:15:32,981]  Epoch 14 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:15:32,983]  Epoch 15 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:15:32,983]  Epoch 15 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:15:32,983]  Epoch 15 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.26it/s]","output_type":"stream"},{"name":"stdout","text":"91.958984375\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:15:47,576]  Epoch 15: training loss is 2.786635890151515.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:15:47,576]  Epoch 15: training loss is 2.786635890151515.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:15:47,576]  Epoch 15: training loss is 2.786635890151515.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:29<00:00, 29.40s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:16:17,284]  Epoch 15: validation BERT F1 = 0.5374505834447013.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:16:17,284]  Epoch 15: validation BERT F1 = 0.5374505834447013.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:16:17,284]  Epoch 15: validation BERT F1 = 0.5374505834447013.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:16:17,288]  Epoch 15 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:16:17,288]  Epoch 15 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:16:17,288]  Epoch 15 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:16:17,290]  Epoch 16 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:16:17,290]  Epoch 16 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:16:17,290]  Epoch 16 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.26it/s]","output_type":"stream"},{"name":"stdout","text":"80.2998046875\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:16:31,883]  Epoch 16: training loss is 2.433327414772727.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:16:31,883]  Epoch 16: training loss is 2.433327414772727.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:16:31,883]  Epoch 16: training loss is 2.433327414772727.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:25<00:00, 25.42s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:16:57,590]  Epoch 16: validation BERT F1 = 0.530038156443172.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:16:57,590]  Epoch 16: validation BERT F1 = 0.530038156443172.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:16:57,590]  Epoch 16: validation BERT F1 = 0.530038156443172.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:16:57,594]  Epoch 16 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:16:57,594]  Epoch 16 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:16:57,594]  Epoch 16 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:16:57,596]  Epoch 17 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:16:57,596]  Epoch 17 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:16:57,596]  Epoch 17 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.31it/s]","output_type":"stream"},{"name":"stdout","text":"75.474609375\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:17:11,909]  Epoch 17: training loss is 2.287109375.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:17:11,909]  Epoch 17: training loss is 2.287109375.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:17:11,909]  Epoch 17: training loss is 2.287109375.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:34<00:00, 34.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:17:46,416]  Epoch 17: validation BERT F1 = 0.5336063769128587.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:17:46,416]  Epoch 17: validation BERT F1 = 0.5336063769128587.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:17:46,416]  Epoch 17: validation BERT F1 = 0.5336063769128587.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:17:46,420]  Epoch 17 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:17:46,420]  Epoch 17 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:17:46,420]  Epoch 17 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:17:46,422]  Epoch 18 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:17:46,422]  Epoch 18 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:17:46,422]  Epoch 18 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.29it/s]","output_type":"stream"},{"name":"stdout","text":"74.1923828125\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:18:00,839]  Epoch 18: training loss is 2.248254024621212.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:18:00,839]  Epoch 18: training loss is 2.248254024621212.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:18:00,839]  Epoch 18: training loss is 2.248254024621212.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:46<00:00, 46.94s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:18:48,091]  Epoch 18: validation BERT F1 = 0.5326210475630231.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:18:48,091]  Epoch 18: validation BERT F1 = 0.5326210475630231.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:18:48,091]  Epoch 18: validation BERT F1 = 0.5326210475630231.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:18:48,095]  Epoch 18 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:18:48,095]  Epoch 18 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:18:48,095]  Epoch 18 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:18:48,098]  Epoch 19 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:18:48,098]  Epoch 19 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:18:48,098]  Epoch 19 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.28it/s]","output_type":"stream"},{"name":"stdout","text":"72.8681640625\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:19:02,617]  Epoch 19: training loss is 2.208126183712121.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:19:02,617]  Epoch 19: training loss is 2.208126183712121.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:19:02,617]  Epoch 19: training loss is 2.208126183712121.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:14<00:00, 14.05s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:19:16,956]  Epoch 19: validation BERT F1 = 0.5121606720818414.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:19:16,956]  Epoch 19: validation BERT F1 = 0.5121606720818414.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:19:16,956]  Epoch 19: validation BERT F1 = 0.5121606720818414.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:19:16,960]  Epoch 19 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:19:16,960]  Epoch 19 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:19:16,960]  Epoch 19 is finished.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:19:16,962]  Epoch 20 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:19:16,962]  Epoch 20 is started.\n2495533887.py[LINE:593]# INFO     [2023-12-31 08:19:16,962]  Epoch 20 is started.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 33/33 [00:14<00:00,  2.28it/s]","output_type":"stream"},{"name":"stdout","text":"77.623046875\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:19:31,481]  Epoch 20: training loss is 2.3522135416666665.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:19:31,481]  Epoch 20: training loss is 2.3522135416666665.\n2495533887.py[LINE:665]# INFO     [2023-12-31 08:19:31,481]  Epoch 20: training loss is 2.3522135416666665.\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 1/1 [00:25<00:00, 25.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"2495533887.py[LINE:676]# INFO     [2023-12-31 08:19:57,484]  Epoch 20: validation BERT F1 = 0.5260635763406754.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:19:57,484]  Epoch 20: validation BERT F1 = 0.5260635763406754.\n2495533887.py[LINE:676]# INFO     [2023-12-31 08:19:57,484]  Epoch 20: validation BERT F1 = 0.5260635763406754.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:19:57,489]  Epoch 20 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:19:57,489]  Epoch 20 is finished.\n2495533887.py[LINE:702]# INFO     [2023-12-31 08:19:57,489]  Epoch 20 is finished.\n","output_type":"stream"}]}]}